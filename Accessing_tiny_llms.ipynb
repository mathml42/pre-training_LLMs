{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052f27a7",
   "metadata": {},
   "source": [
    "## Why we need pretraining?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e7e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33630c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def fix_torch_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_torch_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9907406",
   "metadata": {},
   "source": [
    "### Loading a general pretrained model TinySolar-248m-4k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1acb2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/TinySolar-248m-4k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88821a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0925 14:15:00.370000 199412 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tiny_general_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map = 'cpu',\n",
    "    dtype = torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e64fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_general_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c0bc89",
   "metadata": {},
   "source": [
    "### Generating text with the general pretrained model TinySolar-248m-4k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f42d75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Today is a beautiful day, and\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2432b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tiny_general_tokenizer(prompt, return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ab30161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer  = TextStreamer(\n",
    "    tiny_general_tokenizer,\n",
    "    skip_prompt = True,\n",
    "    skip_special_tokens = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7927f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is a beautiful day, and the weather is perfect for a relaxing weekend.\n",
      "The weather in this area is quite mild, but it's not too hot or cold. The temperature is around 10 degrees Celsius (25-30 degrees Fahrenheit).\n",
      "The climate in this region is very dry, so you can expect to see rainfall during the week. However, if you are planning on visiting the area during the week, be sure to check out the weather forecast for the rest of the week.\n",
      "This is one of the most popular tourist destinations in the world. It is located in the northwest of France, and\n"
     ]
    }
   ],
   "source": [
    "outputs = tiny_general_model.generate(\n",
    "    **inputs,\n",
    "    streamer = streamer,\n",
    "    use_cache = True,\n",
    "    max_new_tokens = 128,\n",
    "    do_sample = False,\n",
    "    temperature = 0.0,\n",
    "    repetition_penalty = 1.1    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae1512",
   "metadata": {},
   "source": [
    "### Try to generate python samples from the above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "126e8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"def find_max(numbers):\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "776573ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tiny_general_tokenizer(prompt, return_tensors = 'pt').to(tiny_general_model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tiny_general_tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e818806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       \"\"\"\n",
      "       Returns the number of times a user has been added to the list.\n",
      "       \"\"\"\n",
      "       return num_users() + 1\n",
      "\n",
      "   def get_user_id(self, id):\n",
      "       \"\"\"\n",
      "       Returns the number of users that have been added to the list.\n",
      "       \"\"\"\n",
      "       return self._get_user_id(id)\n",
      "\n",
      "   def get_user_name(self, name):\n",
      "       \"\"\"\n",
      "       Returns the name of the user that has been added to the list.\n",
      "       \"\"\"\n",
      "       return self._get_user_name(name\n"
     ]
    }
   ],
   "source": [
    "outputs = tiny_general_model.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    temperature = 0.0,\n",
    "    repetition_penalty = 1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5051b",
   "metadata": {},
   "source": [
    "### Try to generate python code using TinySolar- code-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/TinySolar-248m-4k-code-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82589328",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map = 'cpu',\n",
    "    dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "tiny_finetuned_tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7315e536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   if len(numbers) == 0:\n",
      "       return \"Invalid input\"\n",
      "   \n",
      "   max_sum = float('-inf')\n",
      "   max_count = 0\n",
      "   \n",
      "   for i in range(len(numbers)):\n",
      "       if numbers[i] > max_sum and numbers[i] < max_count:\n",
      "           max_sum = numbers[i]\n",
      "           max_count += 1\n",
      "   \n",
      "   return max_sum, max_count\n",
      "```\n",
      "\n",
      "The `find_max()` function takes a list of numbers as input. It first\n"
     ]
    }
   ],
   "source": [
    "prompt = \"def find_max(numbers):\"\n",
    "\n",
    "inputs = tiny_finetuned_tokenizer(prompt, return_tensors='pt').to(tiny_finetuned_model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tiny_finetuned_tokenizer,\n",
    "    skip_prompt= True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = tiny_finetuned_model.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    "    do_sample = False,\n",
    "    temperature = 0.0,\n",
    "    repetition_penalty = 1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe91bf",
   "metadata": {},
   "source": [
    "### Try to generate python code using TinySolar-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0435422",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = \"models/TinySolar-248m-4k-py\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28d337fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_custom_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    device_map=\"cpu\",\n",
    "    dtype=torch.bfloat16,    \n",
    ")\n",
    "\n",
    "tiny_custom_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path_or_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f873acc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   \"\"\"Find the maximum number of numbers in a list.\"\"\"\n",
      "   max = 0\n",
      "   for num in numbers:\n",
      "       if num > max:\n",
      "           max = num\n",
      "   return max\n",
      "\n",
      "\n",
      "def get_min_max(numbers):\n",
      "   \"\"\"Get the minimum number of numbers\n"
     ]
    }
   ],
   "source": [
    "prompt = \"def find_max(numbers):\"\n",
    "\n",
    "inputs = tiny_custom_tokenizer(\n",
    "    prompt, return_tensors=\"pt\"\n",
    ").to(tiny_custom_model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tiny_custom_tokenizer,\n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = tiny_custom_model.generate(\n",
    "    **inputs, streamer=streamer,\n",
    "    use_cache=True, \n",
    "    max_new_tokens=64, \n",
    "    do_sample=False, \n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe69d2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max(numbers):\n",
    "    max = 0\n",
    "    for num in numbers:\n",
    "        if num > max:\n",
    "            max = num\n",
    "    return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7420bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [34,5,7,5,3,1,5,8,43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0651f876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_max(A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
