{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c2c015",
   "metadata": {},
   "source": [
    "## Preparing your model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c884a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore insignificant warnings (ex: deprecation warnings)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a seed value for reproducibility\n",
    "import torch\n",
    "\n",
    "def fix_torch_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_torch_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b268e84",
   "metadata": {},
   "source": [
    "### Model configuration\n",
    "\n",
    "Start by creating a `LlamaConfig` object to configure the architecture of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54935ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig\n",
    "config = LlamaConfig()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a385b47",
   "metadata": {},
   "source": [
    "Next, update parameters to change the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab414023",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.num_hidden_layers = 12      # reduced from 32 to 12\n",
    "config.hidden_size = 1024          # reduced 1/4 from 4096 to 1024\n",
    "config.intermediate_size = 4096    # reduced 1/3 from 11008 to 4096 (dimension of MLP representations)\n",
    "config.num_key_value_heads = 8     # reduced 1/4 from 32 to 8 (defaults to num_attention_heads=32)\n",
    "config.torch_dtype = \"bfloat16\"    # for half-precision training\n",
    "config.use_cache = False           # `True` is incompatible w/ gradient checkpointing\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7266c6",
   "metadata": {},
   "source": [
    "### Weight initialization\n",
    "\n",
    "* Random weight initialization\n",
    "* Using an existing model for continued pre-training\n",
    "* Downscaling an existing model\n",
    "* Upscaling an existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711acd6",
   "metadata": {},
   "source": [
    "#### Random weight initialization\n",
    "\n",
    "Randomly initializing model weights sets all weights to values from a truncated normal distribution with mean 0 and standard deviation of 0.02. Values beyond 2-sigma from the mean are set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "model = LlamaForCausalLM(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nparams(model):\n",
    "    \"\"\"Calculate the total number of model parameters\"\"\"\n",
    "    nparams = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"The total number of parameters is: {nparams}\")\n",
    "\n",
    "print_nparams(model)  # 248013824 => 248M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e8719",
   "metadata": {},
   "source": [
    "Take a look at a sample of the weights in a single layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d478613",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = \"model.layers.0.self_attn.q_proj.weight\"\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name == layer_name:\n",
    "        print(f\"First 30 weights of layer '{layer_name}':\")\n",
    "        print(param.data.view(-1)[:30])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2a342",
   "metadata": {},
   "source": [
    "Try using the model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tokenizer from Upstage Solar, \n",
    "# which is compatible with the Llama-2 tokenizer\n",
    "from transformers import LlamaTokenizer\n",
    "model_dir = \"./models/SOLAR-10.7B-v1.0\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Run simple inference with prompt\n",
    "from transformers import TextStreamer\n",
    "\n",
    "prompt = \"I am an engineer. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    streamer=streamer, \n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e057dc3",
   "metadata": {},
   "source": [
    "Remove the model from memory to avoid crashing the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccb64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "import gc\n",
    "del model\n",
    "del streamer\n",
    "del outputs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88a06e",
   "metadata": {},
   "source": [
    "#### Reuse general pretrained model weights\n",
    "\n",
    "If you load an existing model, you can use it as is to continue pretraining on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name_or_path = \"./models/TinySolar-248m-4k\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a18b0",
   "metadata": {},
   "source": [
    "Remove the model from memory to avoid crashing the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f55f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf938d4",
   "metadata": {},
   "source": [
    "#### Downscaling from a general pretrained model\n",
    "\n",
    "Here we'll downscale the tinySolar-248m-4k model from a 12 layer model to a 10 layer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfa67a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "model_name_or_path = \"./models/TinySolar-248m-4k\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bf8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb667a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nparams(model)  # 248013824 => 248M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ea7cc",
   "metadata": {},
   "source": [
    "Remove the middle two layers (layers 5 and 6) and update the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e29382",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = model.model.layers\n",
    "model.model.layers = layers[:5] + layers[-5:]\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,    \n",
    "    num_hidden_layers=len(model.model.layers),\n",
    ")\n",
    "model.config = config\n",
    "\n",
    "print_nparams(model)  # 217601024 => 217M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a9f2d5",
   "metadata": {},
   "source": [
    "Clear the memory to avoid crashing the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef3cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "import gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c14d2",
   "metadata": {},
   "source": [
    "#### Depth Upscaling from a general pretrained model\n",
    "\n",
    "* Configure a 16 layer model and initialize it with random weights\n",
    "* Load the 12 layer tinySolar-248m-4k model into memory\n",
    "* Copy the bottom 8 and top 8 layers from the 12 layer model and use them to overwrite the random weights of the 16 layer model\n",
    "* Copy over the embedding and classifying layers to replace the randomly initialized counterparts in the 16 layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef085932",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LlamaConfig(\n",
    "    num_hidden_layers=16,  # We want our model to have 16 final layers\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=4096,\n",
    "    num_attention_heads=32,\n",
    "    num_key_value_heads=8,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    use_cache=False \n",
    ")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6032c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM(config)\n",
    "model = model.to(dtype=torch.bfloat16)  # convert to bfloat16\n",
    "print_nparams(model)  # 308839424 => 308M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"upstage/TinySolar-248m-4k\"\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,    \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "print_nparams(pretrained_model) #  248013824 => 248M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "model.model.layers = deepcopy(pretrained_model.model.layers[:-4]) \\\n",
    "    + deepcopy(pretrained_model.model.layers[4:])\n",
    "\n",
    "model.model.embed_tokens = deepcopy(pretrained_model.model.embed_tokens)\n",
    "\n",
    "model.lm_head = deepcopy(pretrained_model.lm_head)\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2838caf",
   "metadata": {},
   "source": [
    "Check the number of parameters is still 308 million:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nparams(model)  # 308839424 => 308M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574c7af",
   "metadata": {},
   "source": [
    "Try using the model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simple inference to show no trained model\n",
    "prompt = \"I am an engineer. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    streamer=streamer, \n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca4225",
   "metadata": {},
   "source": [
    "### Save the model to disk\n",
    "\n",
    "Note the new model name here which reflects the 308 million parameters of the new, upscaled model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dbad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./data/TinySolar-308m-4k-init')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
